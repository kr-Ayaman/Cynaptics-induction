{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-01-17T18:50:11.078154Z\",\"iopub.status.idle\":\"2025-01-17T18:50:11.078671Z\",\"shell.execute_reply\":\"2025-01-17T18:50:11.078429Z\"}}\nimport torch\nfrom torch import nn\nfrom tqdm.auto import tqdm\nfrom torch.autograd import grad\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom torchvision.utils import make_grid\nfrom torchvision import datasets\nfrom torch.utils.data import DataLoader \nimport os\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\ntorch.manual_seed(0) \n\ndef show_tensor_images(image_tensor, num_images=25, size=(3, 64, 64), nrow=5):\n    image_tensor = (image_tensor + 1) / 2\n    image_unflat = image_tensor.detach().cpu()\n    image_grid = make_grid(image_unflat[:num_images], nrow=nrow)\n    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n    plt.show()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-01-17T18:50:11.079933Z\",\"iopub.status.idle\":\"2025-01-17T18:50:11.080491Z\",\"shell.execute_reply\":\"2025-01-17T18:50:11.080260Z\"}}\ndef get_noise(n_samples, z_dim, device='cpu'):\n    return torch.randn(n_samples, z_dim, device=device)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-01-17T18:50:11.082120Z\",\"iopub.status.idle\":\"2025-01-17T18:50:11.082557Z\",\"shell.execute_reply\":\"2025-01-17T18:50:11.082387Z\"}}\nclass Generator(nn.Module):\n    def __init__(self, z_dim):\n        super().__init__()\n        self.z_dim=z_dim\n        self.model = nn.Sequential(\n            nn.ConvTranspose2d(z_dim, 1024, 4, 1, 0),\n            nn.BatchNorm2d(1024),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(1024, 512, 4, 2, 1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(512, 256, 4, 2, 1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(256, 128, 4, 2, 1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(64, img_channels, 4, 4, 0),\n            nn.Tanh()\n        )\n\n    def unsqueeze_noise(self, noise):\n        return noise.view(len(noise), self.z_dim, 1, 1)\n        \n    def forward(self, noise):\n        x = self.unsqueeze_noise(noise)\n        return self.model(x)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-01-17T18:50:11.084649Z\",\"iopub.status.idle\":\"2025-01-17T18:50:11.085140Z\",\"shell.execute_reply\":\"2025-01-17T18:50:11.084998Z\"}}\ngen = nn.DataParallel(Generator(z_dim).to(device))\ngen.load_state_dict(torch.load(\"/models/generator_epoch_100.pth\", map_location=device))\n\n# %% [code]\nshow_tensor_images(gen(get_noise(25, z_dim, device=device)))","metadata":{"_uuid":"259fb679-0153-4f9a-9235-ac385e7734de","_cell_guid":"4efcd672-c9e7-4bc0-b347-c30ea125a61d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}